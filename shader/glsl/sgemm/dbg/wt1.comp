// Resolve SMEM bank-conflict

#version 460

#extension GL_EXT_control_flow_attributes : enable

layout (local_size_x_id = 0, local_size_y_id = 1) in;

layout (binding = 0, std430) readonly buffer SrcMatA { vec4 srcMatA[]; };
layout (binding = 1, std430) readonly buffer SrcMatB { vec4 srcMatB[]; };
layout (binding = 2, std430) writeonly buffer DstMat { vec4 dstMat[]; };

layout (constant_id = 2) const uint M = 0;
layout (constant_id = 3) const uint N = 0;
layout (constant_id = 4) const uint K = 0;
layout (constant_id = 5) const uint BLOCK_TILE_M = 128;
layout (constant_id = 6) const uint BLOCK_TILE_N = 128;
layout (constant_id = 7) const uint BLOCK_TILE_K = 128;
layout (constant_id = 8) const uint THREAD_TILE_M = 16;
layout (constant_id = 9) const uint THREAD_TILE_N = 16;
layout (constant_id = 10) const uint THREAD_TILE_K = 16;
layout (constant_id = 11) const uint THREAD_SUBTILE_M = 8;
layout (constant_id = 12) const uint THREAD_SUBTILE_N = 8;
layout (constant_id = 13) const uint THREAD_SUBTILE_K = 8;
const uint THREAD_TILE_N_VEC = THREAD_TILE_N / 4;
const uint THREAD_TILE_K_VEC = THREAD_TILE_K / 4;
const uint THREAD_SUBTILE_N_VEC = THREAD_SUBTILE_N / 4;
const uint THREAD_SUBTILE_K_VEC = THREAD_SUBTILE_K / 4;

const uvec2 SHARED_EXTENT_A = uvec2(BLOCK_TILE_K / 4, BLOCK_TILE_M);
const uvec2 SHARED_EXTENT_B = uvec2(BLOCK_TILE_N / 4, BLOCK_TILE_K);
shared vec4 sharedA[BLOCK_TILE_M][BLOCK_TILE_K / 4];
shared vec4 sharedB[BLOCK_TILE_K][BLOCK_TILE_N / 4];

void zeroFillAccumulator(inout vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    [[unroll]] for (uint tm = 0; tm < THREAD_TILE_M; tm++) {
        [[unroll]] for (uint tn = 0; tn < THREAD_TILE_N_VEC; tn++) {
            regAccumulator[tm][tn] = vec4(0.0f);
        }
    }
}

void loadGlobalToShared(uvec2 globalCoord, uvec2 globalExtent, uint globalRowStride, bool loadA) {
    const uvec2 groupSize = gl_WorkGroupSize.xy;
    const uint localIndex = gl_LocalInvocationIndex;

    const uint groupThreadCount = groupSize.x * groupSize.y;
    const uint loadsPerThread = globalExtent.x * globalExtent.y / groupThreadCount;

    for (uint i = 0; i < loadsPerThread; i++) {
        const uint linearIdx = i * groupThreadCount + localIndex;
        const uvec2 srcOffset = uvec2(linearIdx % globalExtent.x, linearIdx / globalExtent.x);
        const uvec2 srcCoord = globalCoord + srcOffset;
        const uint srcIndex = srcCoord.y * globalRowStride + srcCoord.x;

        const uvec2 dstCoord = srcOffset;
        if (loadA) {
            sharedA[dstCoord.y][dstCoord.x] = srcMatA[srcIndex];
        } else {
            sharedB[dstCoord.y][dstCoord.x] = srcMatB[srcIndex];
        }
    }
}

void computeWithShared(inout vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    const uvec2 groupSize = gl_WorkGroupSize.xy;
    const uvec2 localID = gl_LocalInvocationID.xy;

    const uint threadSplitKCount = BLOCK_TILE_K / THREAD_TILE_K;

    const uint subTileCountM = THREAD_TILE_M / THREAD_SUBTILE_M;
    const uint subTileCountN = THREAD_TILE_N / THREAD_SUBTILE_N;
    const uint subTileCountK = THREAD_TILE_K / THREAD_SUBTILE_K;

    vec4 regA[THREAD_SUBTILE_K_VEC];                       // THREAD_SUBTILE_K / 4
    vec4 regB[THREAD_SUBTILE_K][THREAD_SUBTILE_N_VEC];     // THREAD_SUBTILE_K × (THREAD_SUBTILE_N / 4)

    // 外层遍历每个 thread-tile 的 subtile
    [[unroll]] for (uint subM_idx = 0; subM_idx < subTileCountM; subM_idx++) {
        [[unroll]] for (uint subN_idx = 0; subN_idx < subTileCountN; subN_idx++) {

            // 遍历 K 方向的所有块
            for (uint iThreadSplitK = 0; iThreadSplitK < threadSplitKCount; iThreadSplitK++) {
                [[unroll]] for (uint subK_idx = 0; subK_idx < subTileCountK; subK_idx++) {
                    const uint baseK_in_tile = subK_idx * THREAD_SUBTILE_K;
                    const uint sharedBaseY = iThreadSplitK * THREAD_TILE_K + baseK_in_tile;

                    // --- Load B 子块 ---
                    const uint subN_sharedStart = subN_idx * THREAD_SUBTILE_N / 4;
                    [[unroll]] for (uint tk = 0; tk < THREAD_SUBTILE_K; tk++) {
                        const uint sharedCoordY = sharedBaseY + tk;
                        [[unroll]] for (uint sn = 0; sn < THREAD_SUBTILE_N_VEC; sn++) {
                            const uint sharedCoordX = (subN_sharedStart + sn) * groupSize.x + localID.x;
                            regB[tk][sn] = sharedB[sharedCoordY][sharedCoordX];
                        }
                    }

                    // --- 遍历 A 子块的每一行 ---
                    [[unroll]] for (uint tm = 0; tm < THREAD_SUBTILE_M; tm++) {
                        const uint globalM = subM_idx * THREAD_SUBTILE_M + tm;
                        const uint sharedCoordY_A = globalM * groupSize.y + localID.y;

                        // 加载 A 子块
                        [[unroll]] for (uint tkvec = 0; tkvec < THREAD_SUBTILE_K_VEC; tkvec++) {
                            const uint sharedCoordX_A =
                                iThreadSplitK * THREAD_TILE_K_VEC + (baseK_in_tile / 4) + tkvec;
                            regA[tkvec] = sharedA[sharedCoordY_A][sharedCoordX_A];
                        }

                        // --- 计算并直接累加到 regAccumulator ---
                        [[unroll]] for (uint sn = 0; sn < THREAD_SUBTILE_N_VEC; sn++) {
                            const uint globalN = subN_idx * THREAD_SUBTILE_N_VEC + sn;
                            [[unroll]] for (uint tkvec = 0; tkvec < THREAD_SUBTILE_K_VEC; tkvec++) {
                                const uint tkBase = tkvec * 4;
                                regAccumulator[globalM][globalN] += regA[tkvec].x * regB[tkBase + 0][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].y * regB[tkBase + 1][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].z * regB[tkBase + 2][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].w * regB[tkBase + 3][sn];
                            }
                        }
                    } // end tm
                } // end subK_idx
            } // end iThreadSplitK
        } // end subN_idx
    } // end subM_idx
}


void storeAccumulator(vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    const uvec2 groupID = gl_WorkGroupID.xy;
    const uvec2 groupSize = gl_WorkGroupSize.xy;
    const uvec2 localID = gl_LocalInvocationID.xy;

    const uvec2 globalCoordBase = uvec2(groupID.x * BLOCK_TILE_N / 4, groupID.y * BLOCK_TILE_M);
    [[unroll]] for (uint tm = 0; tm < THREAD_TILE_M; tm++) {
        [[unroll]] for (uint tn = 0; tn < THREAD_TILE_N_VEC; tn++) {
            const uint globalCoordY = globalCoordBase.y + tm * groupSize.y + localID.y;
            const uint globalCoordX = globalCoordBase.x + tn * groupSize.x + localID.x;
            const uint dstIdx = globalCoordY * N / 4 + globalCoordX;
            dstMat[dstIdx] = regAccumulator[tm][tn];
        }
    }
}

void main() {
    const uvec2 groupID = gl_WorkGroupID.xy;

    // Accumulator
    vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC];
    zeroFillAccumulator(regAccumulator);

    // Main-loop
    const uint blockBaseM = groupID.y * BLOCK_TILE_M;
    const uint blockBaseN = groupID.x * BLOCK_TILE_N;
    const uint blockSplitKCount = K / BLOCK_TILE_K;
    for (uint iBlockSplitK = 0; iBlockSplitK < blockSplitKCount; iBlockSplitK++) {
        const uint blockBaseK = iBlockSplitK * BLOCK_TILE_K;

        const uvec2 globalCoordA = uvec2(blockBaseK / 4, blockBaseM);
        loadGlobalToShared(globalCoordA, SHARED_EXTENT_A, K / 4, true);// Load A

        const uvec2 globalCoordB = uvec2(blockBaseN / 4, blockBaseK);
        loadGlobalToShared(globalCoordB, SHARED_EXTENT_B, N / 4, false);// Load B
        barrier();

        computeWithShared(regAccumulator);
        barrier();
    }

    // Store results
    storeAccumulator(regAccumulator);
}
