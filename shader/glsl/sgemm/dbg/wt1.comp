// Resolve SMEM bank-conflict

#version 460

#extension GL_EXT_control_flow_attributes : enable

layout (local_size_x_id = 0, local_size_y_id = 1) in;

layout (binding = 0, std430) readonly buffer SrcMatA { vec4 srcMatA[]; };
layout (binding = 1, std430) readonly buffer SrcMatB { vec4 srcMatB[]; };
layout (binding = 2, std430) writeonly buffer DstMat { vec4 dstMat[]; };

layout (constant_id = 2) const int M = 0;
layout (constant_id = 3) const int N = 0;
layout (constant_id = 4) const int K = 0;
layout (constant_id = 5) const int BLOCK_TILE_M = 128;
layout (constant_id = 6) const int BLOCK_TILE_N = 128;
layout (constant_id = 7) const int BLOCK_TILE_K = 128;
layout (constant_id = 8) const int THREAD_TILE_M = 16;
layout (constant_id = 9) const int THREAD_TILE_N = 16;
layout (constant_id = 10) const int THREAD_TILE_K = 16;
layout (constant_id = 11) const int THREAD_SUBTILE_M = 8;
layout (constant_id = 12) const int THREAD_SUBTILE_N = 8;
layout (constant_id = 13) const int THREAD_SUBTILE_K = 8;
const int THREAD_TILE_N_VEC = THREAD_TILE_N / 4;
const int THREAD_TILE_K_VEC = THREAD_TILE_K / 4;
const int THREAD_SUBTILE_N_VEC = THREAD_SUBTILE_N / 4;
const int THREAD_SUBTILE_K_VEC = THREAD_SUBTILE_K / 4;

const ivec2 SHARED_EXTENT_A = ivec2(BLOCK_TILE_K / 4, BLOCK_TILE_M);
const ivec2 SHARED_EXTENT_B = ivec2(BLOCK_TILE_N / 4, BLOCK_TILE_K);
shared vec4 sharedA[BLOCK_TILE_M][BLOCK_TILE_K / 4];
shared vec4 sharedB[BLOCK_TILE_K][BLOCK_TILE_N / 4];

void zeroFillAccumulator(inout vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    [[unroll]] for (int tm = 0; tm < THREAD_TILE_M; tm++) {
        [[unroll]] for (int tn = 0; tn < THREAD_TILE_N_VEC; tn++) {
            regAccumulator[tm][tn] = vec4(0.0f);
        }
    }
}

void loadGlobalToShared(ivec2 globalCoord, ivec2 globalExtent, int globalRowStride, bool loadA) {
    const ivec2 groupSize = ivec2(gl_WorkGroupSize.xy);
    const int localIndex = int(gl_LocalInvocationIndex);

    const int groupThreadCount = int(groupSize.x * groupSize.y);
    const int loadsPerThread = globalExtent.x * globalExtent.y / groupThreadCount;

    for (int i = 0; i < loadsPerThread; i++) {
        const int linearIdx = i * groupThreadCount + localIndex;
        const ivec2 srcOffset = ivec2(linearIdx % globalExtent.x, linearIdx / globalExtent.x);
        const ivec2 srcCoord = globalCoord + srcOffset;
        const int srcIndex = srcCoord.y * globalRowStride + srcCoord.x;

        const ivec2 dstCoord = srcOffset;
        if (loadA) {
            sharedA[dstCoord.y][dstCoord.x] = srcMatA[srcIndex];
        } else {
            sharedB[dstCoord.y][dstCoord.x] = srcMatB[srcIndex];
        }
    }
}

void computeWithShared(inout vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    const ivec2 groupSize = ivec2(gl_WorkGroupSize.xy);
    const ivec2 localID = ivec2(gl_LocalInvocationID.xy);

    const int threadSplitKCount = BLOCK_TILE_K / THREAD_TILE_K;

    const int subTileCountM = THREAD_TILE_M / THREAD_SUBTILE_M;
    const int subTileCountN = THREAD_TILE_N / THREAD_SUBTILE_N;
    const int subTileCountK = THREAD_TILE_K / THREAD_SUBTILE_K;

    // 小寄存器缓存
    vec4 regA[THREAD_SUBTILE_K_VEC];                       // THREAD_SUBTILE_K / 4
    vec4 regB[THREAD_SUBTILE_K][THREAD_SUBTILE_N_VEC];     // THREAD_SUBTILE_K × (THREAD_SUBTILE_N / 4)

    // 外层遍历每个 thread-tile 的 subtile
    [[unroll]] for (int subM_idx = 0; subM_idx < subTileCountM; subM_idx++) {
        [[unroll]] for (int subN_idx = 0; subN_idx < subTileCountN; subN_idx++) {

            // 遍历 K 方向的所有块
            for (int iThreadSplitK = 0; iThreadSplitK < threadSplitKCount; iThreadSplitK++) {
                [[unroll]] for (int subK_idx = 0; subK_idx < subTileCountK; subK_idx++) {
                    const int baseK_in_tile = subK_idx * THREAD_SUBTILE_K;
                    const int sharedBaseY = iThreadSplitK * THREAD_TILE_K + baseK_in_tile;

                    // --- Load B 子块 ---
                    const int subN_sharedStart = subN_idx * THREAD_SUBTILE_N / 4;
                    [[unroll]] for (int tk = 0; tk < THREAD_SUBTILE_K; tk++) {
                        const int sharedCoordY = sharedBaseY + tk;
                        [[unroll]] for (int sn = 0; sn < THREAD_SUBTILE_N_VEC; sn++) {
                            const int sharedCoordX = (subN_sharedStart + sn) * groupSize.x + localID.x;
                            regB[tk][sn] = sharedB[sharedCoordY][sharedCoordX];
                        }
                    }

                    // --- 遍历 A 子块的每一行 ---
                    [[unroll]] for (int tm = 0; tm < THREAD_SUBTILE_M; tm++) {
                        const int globalM = subM_idx * THREAD_SUBTILE_M + tm;
                        const int sharedCoordY_A = globalM * groupSize.y + localID.y;

                        // 加载 A 子块
                        [[unroll]] for (int tkvec = 0; tkvec < THREAD_SUBTILE_K_VEC; tkvec++) {
                            const int sharedCoordX_A =
                                iThreadSplitK * THREAD_TILE_K_VEC + (baseK_in_tile / 4) + tkvec;
                            regA[tkvec] = sharedA[sharedCoordY_A][sharedCoordX_A];
                        }

                        // --- 计算并直接累加到 regAccumulator ---
                        [[unroll]] for (int sn = 0; sn < THREAD_SUBTILE_N_VEC; sn++) {
                            const int globalN = subN_idx * THREAD_SUBTILE_N_VEC + sn;
                            [[unroll]] for (int tkvec = 0; tkvec < THREAD_SUBTILE_K_VEC; tkvec++) {
                                const int tkBase = tkvec * 4;
                                regAccumulator[globalM][globalN] += regA[tkvec].x * regB[tkBase + 0][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].y * regB[tkBase + 1][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].z * regB[tkBase + 2][sn];
                                regAccumulator[globalM][globalN] += regA[tkvec].w * regB[tkBase + 3][sn];
                            }
                        }
                    } // end tm
                } // end subK_idx
            } // end iThreadSplitK
        } // end subN_idx
    } // end subM_idx
}


void storeAccumulator(vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC]) {
    const ivec2 groupID = ivec2(gl_WorkGroupID.xy);
    const ivec2 groupSize = ivec2(gl_WorkGroupSize.xy);
    const ivec2 localID = ivec2(gl_LocalInvocationID.xy);

    const ivec2 globalCoordBase = ivec2(groupID.x * BLOCK_TILE_N / 4, groupID.y * BLOCK_TILE_M);
    [[unroll]] for (int tm = 0; tm < THREAD_TILE_M; tm++) {
        [[unroll]] for (int tn = 0; tn < THREAD_TILE_N_VEC; tn++) {
            const int globalCoordY = globalCoordBase.y + tm * groupSize.y + localID.y;
            const int globalCoordX = globalCoordBase.x + tn * groupSize.x + localID.x;
            const int dstIdx = globalCoordY * N / 4 + globalCoordX;
            dstMat[dstIdx] = regAccumulator[tm][tn];
        }
    }
}

void main() {
    const ivec2 groupID = ivec2(gl_WorkGroupID.xy);

    // Accumulator
    vec4 regAccumulator[THREAD_TILE_M][THREAD_TILE_N_VEC];
    zeroFillAccumulator(regAccumulator);

    // Main-loop
    const int blockBaseM = groupID.y * BLOCK_TILE_M;
    const int blockBaseN = groupID.x * BLOCK_TILE_N;
    const int blockSplitKCount = K / BLOCK_TILE_K;
    for (int iBlockSplitK = 0; iBlockSplitK < blockSplitKCount; iBlockSplitK++) {
        const int blockBaseK = iBlockSplitK * BLOCK_TILE_K;

        const ivec2 globalCoordA = ivec2(blockBaseK / 4, blockBaseM);
        loadGlobalToShared(globalCoordA, SHARED_EXTENT_A, K / 4, true);// Load A

        const ivec2 globalCoordB = ivec2(blockBaseN / 4, blockBaseK);
        loadGlobalToShared(globalCoordB, SHARED_EXTENT_B, N / 4, false);// Load B
        barrier();

        computeWithShared(regAccumulator);
        barrier();
    }

    // Store results
    storeAccumulator(regAccumulator);
}
